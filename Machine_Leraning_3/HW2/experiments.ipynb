{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm.notebook import tqdm \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_simple = T.Compose([\n",
    "    T.ToTensor(), \n",
    "])\n",
    "\n",
    "train_folder = ImageFolder('tiny-imagenet-200/train', transform=transforms_simple)\n",
    "val_folder = ImageFolder('tiny-imagenet-200/val', transform=transforms_simple)\n",
    "test_folder = ImageFolder('tiny-imagenet-200/test', transform=transforms_simple)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_folder,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=16\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_folder,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=16\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_folder,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/denis/Documents/AI_Masters/Machine_Leraning_3/HW2/experiments.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denis/Documents/AI_Masters/Machine_Leraning_3/HW2/experiments.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m mean_sq \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39m3\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denis/Documents/AI_Masters/Machine_Leraning_3/HW2/experiments.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/denis/Documents/AI_Masters/Machine_Leraning_3/HW2/experiments.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m x, _ \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denis/Documents/AI_Masters/Machine_Leraning_3/HW2/experiments.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     mean \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(x, dim\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denis/Documents/AI_Masters/Machine_Leraning_3/HW2/experiments.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     mean_sq \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(x \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rlock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[39m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[39mreturn\u001b[39;00m _ForkingPickler\u001b[39m.\u001b[39;49mloads(res)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:307\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrebuild_storage_fd\u001b[39m(\u001b[39mcls\u001b[39m, df, size):\n\u001b[0;32m--> 307\u001b[0m     fd \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mdetach()\n\u001b[1;32m    308\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m         storage \u001b[39m=\u001b[39m storage_from_cache(\u001b[39mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetach\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mwith\u001b[39;00m _resource_sharer\u001b[39m.\u001b[39;49mget_connection(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id) \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[39mreturn\u001b[39;00m reduction\u001b[39m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[39m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[39m=\u001b[39m Client(address, authkey\u001b[39m=\u001b[39;49mprocess\u001b[39m.\u001b[39;49mcurrent_process()\u001b[39m.\u001b[39;49mauthkey)\n\u001b[1;32m     87\u001b[0m c\u001b[39m.\u001b[39msend((key, os\u001b[39m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/multiprocessing/connection.py:502\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    500\u001b[0m     c \u001b[39m=\u001b[39m PipeClient(address)\n\u001b[1;32m    501\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 502\u001b[0m     c \u001b[39m=\u001b[39m SocketClient(address)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m authkey \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(authkey, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m    505\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mauthkey should be a byte string\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/multiprocessing/connection.py:630\u001b[0m, in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mwith\u001b[39;00m socket\u001b[39m.\u001b[39msocket( \u001b[39mgetattr\u001b[39m(socket, family) ) \u001b[39mas\u001b[39;00m s:\n\u001b[1;32m    629\u001b[0m     s\u001b[39m.\u001b[39msetblocking(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 630\u001b[0m     s\u001b[39m.\u001b[39;49mconnect(address)\n\u001b[1;32m    631\u001b[0m     \u001b[39mreturn\u001b[39;00m Connection(s\u001b[39m.\u001b[39mdetach())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean = torch.zeros(3, dtype=float)\n",
    "mean_sq = torch.zeros(3, dtype=float)\n",
    "n = len(train_loader)\n",
    "\n",
    "for x, _ in train_loader:\n",
    "    mean += torch.mean(x, dim=(0, 2, 3))\n",
    "    mean_sq += torch.mean(x ** 2, dim=(0, 2, 3))\n",
    "\n",
    "mean /= n\n",
    "mean_sq /= n\n",
    "std = torch.sqrt(mean_sq - mean ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor, mean=(0.5, 0.5, 0.5), std=(0.25, 0.25, 0.25)):\n",
    "    # mean, std ->\n",
    "    mean = torch.tensor(mean)[:, None, None]\n",
    "    std = torch.tensor(std)[:, None, None]\n",
    "\n",
    "    renormalized_tensor = mean + std * tensor\n",
    "    image = renormalized_tensor.permute(1, 2, 0).detach().cpu().numpy()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_folder.class_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.GoogLeNet(\n",
    "    num_classes=num_classes,\n",
    "    aux_logits=True,\n",
    "    dropout_aux=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.conv1 = nn.Conv2d(3, 192, kernel_size=7, stride=1, padding=0)\n",
    "model.conv1 = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=5, stride=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 192, kernel_size=5, stride=1)\n",
    ")\n",
    "model.conv2 = nn.Identity()\n",
    "model.conv3 = nn.Identity()\n",
    "model.maxpool2 = nn.Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor, mean=(0.5, 0.5, 0.5), std=(0.25, 0.25, 0.25)):\n",
    "    # mean, std ->\n",
    "    mean = torch.tensor(mean)[:, None, None]\n",
    "    std = torch.tensor(std)[:, None, None]\n",
    "\n",
    "    renormalized_tensor = mean + std * tensor\n",
    "    image = renormalized_tensor.permute(1, 2, 0).detach().cpu().numpy()\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, filename):\n",
    "\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        torch.save(model.state_dict(), fp)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, filename):\n",
    "\n",
    "    with open(filename, \"rb\") as fp:\n",
    "        state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter1 = iter(train_loader)\n",
    "iter2 = iter(train_loader)\n",
    "x1, y1 = next(iter1)\n",
    "x2, y2 = next(iter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUp:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, x1, x2):\n",
    "        return x1 * self.alpha + x2 * (1 - self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup = MixUp(0.5)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].imshow(tensor_to_image(x1[0]))\n",
    "axs[1].imshow(tensor_to_image(x2[0]))\n",
    "axs[2].imshow(tensor_to_image(mixup(x1, x2)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCatOut:\n",
    "    \n",
    "    def __init__(self, size, prob):\n",
    "        self.size = size\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sample = sample.clone().detach()\n",
    "\n",
    "        pos_x = randint(0, 64 - self.size)\n",
    "        pos_y = randint(0, 64 - self.size)\n",
    "\n",
    "        if np.random.binomial(n=1, p=self.prob, size=1)[0] == 1:\n",
    "            sample[:, pos_x: pos_x + self.size, pos_y: pos_y + self.size] = 0\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catout = RandomCatOut(24, 1)\n",
    "plt.imshow(tensor_to_image(catout(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CutMix:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, x1, x2):\n",
    "        x1 = x1.clone()\n",
    "        x2 = x2.clone()\n",
    "\n",
    "        pos1_x = randint(0, 64 - self.size)\n",
    "        pos1_y = randint(0, 64 - self.size)\n",
    "\n",
    "        pos2_x = randint(0, 64 - self.size)\n",
    "        pos2_y = randint(0, 64 - self.size)\n",
    "\n",
    "        x1[:, :, pos1_x: pos1_x + self.size, pos1_y: pos1_y + self.size] = \\\n",
    "            x2[:, :, pos2_x: pos2_x + self.size, pos2_y: pos2_y + self.size]\n",
    "        \n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutmix = CutMix(size=32)\n",
    "x3 = cutmix(x1, x2)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 3))\n",
    "axs[0].imshow(tensor_to_image(x1[2]))\n",
    "axs[1].imshow(tensor_to_image(x2[2]))\n",
    "axs[2].imshow(tensor_to_image(x3[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0.05,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 15,\n",
    "    \"optimizer\": torch.optim.AdamW,\n",
    "    \"label_smooth\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomApply(\n",
    "        torch.nn.ModuleList([T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3)]),\n",
    "        p=0.3\n",
    "    ),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomApply(\n",
    "        torch.nn.ModuleList([T.RandomRotation(degrees=15)]),\n",
    "        p=0.3\n",
    "    ),\n",
    "    T.RandomApply(\n",
    "        torch.nn.ModuleList([\n",
    "            T.RandomCrop(size=48),\n",
    "            T.Resize(size=64, antialias=True),\n",
    "        ]),\n",
    "        p=0.3\n",
    "    ),\n",
    "    RandomCatOut(size=24, prob=0.3),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "transforms_val = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "\n",
    "train_folder = ImageFolder('tiny-imagenet-200/train', transform=transforms_train)\n",
    "val_folder = ImageFolder('tiny-imagenet-200/val', transform=transforms_val)\n",
    "test_folder = ImageFolder('tiny-imagenet-200/test', transform=transforms_val)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_folder,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_folder,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_folder,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint(model, 'checkpoints_SwallowTrained/epoch=12_valloss=1.675.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"hw2\",\n",
    "    name=\"Swallow Trained SmoothLabel Softmax\",\n",
    "    reinit=True,\n",
    "    # track hyperparameters and run metadata\n",
    "    config=config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reg_epoch(stage, model, dataloader, loss_fn, optimizer, sheduler, epoch, device):\n",
    "    losses = []\n",
    "    for batch in tqdm(dataloader, total=len(dataloader), desc=f\"epoch: {str(epoch).zfill(3)} | {stage:5}\"):\n",
    "        xs, ys_true = batch\n",
    "\n",
    "        # ys_pred = model(xs.to(device))\n",
    "        # loss = loss_fn(ys_pred, ys_true.to(device))\n",
    "\n",
    "        ys_true = ys_true.to(device)\n",
    "\n",
    "        output = model(xs.to(device))\n",
    "\n",
    "        if stage == \"train\":\n",
    "            o1 = output.aux_logits1.softmax(dim=1)\n",
    "            o2 = output.aux_logits2.softmax(dim=1)\n",
    "            o3 = output.logits.softmax(dim=1)\n",
    "\n",
    "            loss1 = loss_fn(o1, ys_true)\n",
    "            loss2 = loss_fn(o2, ys_true)\n",
    "            loss3 = loss_fn(o3, ys_true)\n",
    "\n",
    "            loss = 0.3 * loss1 + 0.3 * loss2 + 0.4 * loss3\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            wandb.log({\"lr\": sheduler.get_last_lr(), \"reg_loss\": loss3})\n",
    "\n",
    "            losses.append(loss3.detach().cpu().item())\n",
    "\n",
    "        else:\n",
    "            loss = loss_fn(output, ys_true)\n",
    "\n",
    "            losses.append(loss.detach().cpu().item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mixup_epoch(stage, model, dataloader, loss_fn, optimizer, sheduler, epoch, device):\n",
    "    losses = []\n",
    "\n",
    "    p_mix = 0.5\n",
    "    mixup = MixUp(p_mix)\n",
    "\n",
    "    for batch1, batch2 in tqdm(zip(dataloader, dataloader), total=len(dataloader), desc=f\"epoch: {str(epoch).zfill(3)} | {stage:5}\"):\n",
    "        xs1, ys1_true = batch1\n",
    "        xs2, ys2_true = batch2\n",
    "\n",
    "        xs = mixup(xs1, xs2)\n",
    "\n",
    "        # ys_pred = model(xs.to(device))\n",
    "        # loss = loss_fn(ys_pred, ys_true.to(device))\n",
    "\n",
    "        ys1_true = ys1_true.to(device)\n",
    "        ys2_true = ys2_true.to(device)\n",
    "\n",
    "        output = model(xs.to(device))\n",
    "\n",
    "        if stage == \"train\":\n",
    "            o1 = output.aux_logits1.softmax(dim=1)\n",
    "            o2 = output.aux_logits2.softmax(dim=1)\n",
    "            o3 = output.logits.softmax(dim=1)\n",
    "\n",
    "            loss1_1 = loss_fn(o1, ys1_true)\n",
    "            loss1_2 = loss_fn(o2, ys1_true)\n",
    "            loss1_3 = loss_fn(o3, ys1_true)\n",
    "\n",
    "            loss2_1 = loss_fn(o1, ys2_true)\n",
    "            loss2_2 = loss_fn(o2, ys2_true)\n",
    "            loss2_3 = loss_fn(o3, ys2_true)\n",
    "\n",
    "            loss1 = 0.3 * loss1_1 + 0.3 * loss1_2 + 0.4 * loss1_3\n",
    "            loss2 = 0.3 * loss2_1 + 0.3 * loss2_2 + 0.4 * loss2_3\n",
    "\n",
    "            loss = loss1 * p_mix + loss2 * (1 - p_mix)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            wandb.log({\n",
    "                \"lr\": sheduler.get_last_lr(), \n",
    "                \"mixup_loss\": (loss2_3 * p_mix + loss2 * (1 - p_mix))\n",
    "            })\n",
    "\n",
    "            losses.append((loss2_3 * p_mix + loss2 * (1 - p_mix)).detach().cpu().item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cutmix_epoch(stage, model, dataloader, loss_fn, optimizer, sheduler, epoch, device):\n",
    "    losses = []\n",
    "\n",
    "    cut_size = 32\n",
    "    cut_p = 1 - (cut_size / 64) ** 2\n",
    "    cutmix = CutMix(size=cut_size)\n",
    "\n",
    "    for batch1, batch2 in tqdm(zip(dataloader, dataloader), total=len(dataloader), desc=f\"epoch: {str(epoch).zfill(3)} | {stage:5}\"):\n",
    "        xs1, ys1_true = batch1\n",
    "        xs2, ys2_true = batch2\n",
    "\n",
    "        xs = cutmix(xs1, xs2)\n",
    "\n",
    "        # ys_pred = model(xs.to(device))\n",
    "        # loss = loss_fn(ys_pred, ys_true.to(device))\n",
    "\n",
    "        ys1_true = ys1_true.to(device)\n",
    "        ys2_true = ys2_true.to(device)\n",
    "\n",
    "        output = model(xs.to(device))\n",
    "\n",
    "        if stage == \"train\":\n",
    "            o1 = output.aux_logits1.softmax(dim=1)\n",
    "            o2 = output.aux_logits2.softmax(dim=1)\n",
    "            o3 = output.logits.softmax(dim=1)\n",
    "\n",
    "            loss1_1 = loss_fn(o1, ys1_true)\n",
    "            loss1_2 = loss_fn(o2, ys1_true)\n",
    "            loss1_3 = loss_fn(o3, ys1_true)\n",
    "\n",
    "            loss2_1 = loss_fn(o1, ys2_true)\n",
    "            loss2_2 = loss_fn(o2, ys2_true)\n",
    "            loss2_3 = loss_fn(o3, ys2_true)\n",
    "\n",
    "            loss1 = 0.3 * loss1_1 + 0.3 * loss1_2 + 0.4 * loss1_3\n",
    "            loss2 = 0.3 * loss2_1 + 0.3 * loss2_2 + 0.4 * loss2_3\n",
    "\n",
    "            loss = loss1 * cut_p + loss2 * (1 - cut_p)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            wandb.log({\n",
    "                \"lr\": sheduler.get_last_lr(), \n",
    "                \"mixcut_loss\": (loss2_3 * cut_p + loss2 * (1 - cut_p))\n",
    "            })\n",
    "\n",
    "            losses.append((loss2_3 * cut_p + loss2 * (1 - cut_p)).detach().cpu().item())\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_super_epoch(stage, model, dataloader, loss_fn, optimizer, sheduler, epoch, device):\n",
    "    # v NOTE THIS v\n",
    "    if stage == \"train\":\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "    else:\n",
    "        torch.set_grad_enabled(False)\n",
    "        model.eval()\n",
    "    # ^ NOTE THIS ^\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # if epoch % 2 == 0 or stage == 'val':\n",
    "    losses = run_reg_epoch(stage, model, dataloader, loss_fn, optimizer, sheduler, epoch, device)\n",
    "\n",
    "    # elif epoch % 3 == 1:\n",
    "    #     losses = run_mixup_epoch(stage, model, dataloader, loss_fn, optimizer, sheduler, epoch, device)\n",
    "    \n",
    "    # elif epoch % 3 == 2:\n",
    "    # else: \n",
    "    #     losses = run_cutmix_epoch(stage, model, dataloader, loss_fn, optimizer, sheduler, epoch, device)\n",
    "    \n",
    "    if stage == \"train\":\n",
    "        sheduler.step()\n",
    "\n",
    "        \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model, dataloader_train, dataloader_val, \n",
    "    loss_fn, optimizer, sheduler, num_epochs, \n",
    "    device, output_dir, start_epoch=0\n",
    "):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    best_val_loss_epoch = -1\n",
    "    best_val_loss_fn = None\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        train_loss = run_super_epoch(\"train\", model, dataloader_train, loss_fn, optimizer, sheduler, epoch, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        val_loss = run_super_epoch(\"val\", model, dataloader_val, loss_fn, optimizer, sheduler, epoch, device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # if epoch % 2 == 0: \n",
    "        train_loss_name = \"epoch_loss_train_reg\"\n",
    "\n",
    "        # if epoch % 3 == 1:\n",
    "        #     train_loss_name = \"epoch_loss_train_mixup\"\n",
    "\n",
    "        # if epoch % 3 == 2:\n",
    "        # else:\n",
    "        #     train_loss_name = \"epoch_loss_train_micxut\"\n",
    "\n",
    "        wandb.log({train_loss_name: train_loss, \"epoch_loss_val\": val_loss, \"epoch\": epoch})\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_loss_epoch = epoch\n",
    "\n",
    "            output_fn = os.path.join(output_dir, f\"epoch={str(epoch).zfill(2)}_valloss={best_val_loss:.3f}.pth.tar\")\n",
    "            save_checkpoint(model, output_fn)\n",
    "            print(f\"New checkpoint saved to {output_fn}\\n\")\n",
    "\n",
    "            best_val_loss_fn = output_fn\n",
    "\n",
    "    print(f\"Best val_loss = {best_val_loss:.3f} reached at epoch {best_val_loss_epoch}\")\n",
    "    load_checkpoint(model, best_val_loss_fn)\n",
    "\n",
    "    return train_losses, val_losses, best_val_loss, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(label_smoothing=config['label_smooth'])\n",
    "optimizer = config[\"optimizer\"](\n",
    "    model.parameters(), \n",
    "    lr=config[\"learning_rate\"], \n",
    "    weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "sheduler = lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses_baseline, val_losses_baseline, best_val_loss_baseline, model = run_experiment(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    sheduler,\n",
    "    config[\"num_epochs\"],\n",
    "    device,\n",
    "    \"checkpoints_SwallowTrainedSmoothed\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.title(title)\n",
    "    plt.plot(train_losses, label=\"train\")\n",
    "    plt.plot(val_losses, label=\"val\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses_baseline, val_losses_baseline, title=\"cnn_baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    labels_all = []\n",
    "    probs_all = []\n",
    "    preds_all = []\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        images, labels = batch\n",
    "\n",
    "        logits = model(images.to(device)).cpu()\n",
    "        probs = logits.softmax(dim=1)\n",
    "        max_prob, max_prob_index = torch.max(probs, dim=1)\n",
    "\n",
    "        labels_all.extend(labels.numpy().tolist())\n",
    "        probs_all.extend(max_prob.numpy().tolist())\n",
    "        preds_all.extend(max_prob_index.numpy().tolist())\n",
    "\n",
    "    return labels_all, probs_all, preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_probs, train_preds = collect_predictions(model, train_loader, device)\n",
    "\n",
    "accuracy_train = accuracy_score(train_labels, train_preds)\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels, test_probs, test_preds = collect_predictions(model, val_loader, device)\n",
    "\n",
    "accuracy_train = accuracy_score(test_labels, test_preds)\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
