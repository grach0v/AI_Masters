{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашняя работа 5: И снова распознавание текста\n",
    "\n",
    "## Грачев Денис Вадимович"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/miniconda3/envs/ml3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/denis/miniconda3/envs/ml3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn            import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear, ReLU, Dropout, LayerNorm\n",
    "from torch.utils.data    import Dataset, DataLoader\n",
    "from torch.nn.functional import ctc_loss, softmax\n",
    "from torchvision         import models\n",
    "from torchvision import transforms as T\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from string import digits, ascii_uppercase\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import gc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./seminar/data/seminar_crnn_data/\"  # Change to your path with unzipped data\n",
    "config_path = os.path.join(PATH_TO_DATA, \"config.json\")\n",
    "images_path = os.path.join(PATH_TO_DATA, \"images\")\n",
    "\n",
    "assert os.path.isfile(config_path)\n",
    "assert os.path.isdir(images_path)\n",
    "\n",
    "with open(config_path, \"rt\") as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "config_full_paths = []\n",
    "for item in config:\n",
    "    config_full_paths.append({\"file\": os.path.join(images_path, item[\"file\"]),\n",
    "                              \"text\": item[\"text\"]})\n",
    "seminar_config = config_full_paths\n",
    "abc = \"0123456789ABEKMHOPCTYX\"  # this is our alphabet for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41141/41141 [00:00<00:00, 384736.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in data after filtering: 31345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_mask(text):\n",
    "    \"\"\"Compute letter-digit mask of text, e.g. 'E506EC152' -> 'LDDDLLDDD'.\n",
    "    \n",
    "    Args:\n",
    "        - text: String of text. \n",
    "        \n",
    "    Returns:\n",
    "        String of the same length but with every letter replaced by 'L' and every digit replaced by 'D' \n",
    "        or None if non-letter and non-digit character met in text.\n",
    "    \"\"\"\n",
    "    mask = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for char in text:\n",
    "        if char in digits:\n",
    "            mask.append(\"D\")\n",
    "        elif char in ascii_uppercase:\n",
    "            mask.append(\"L\")\n",
    "        else:\n",
    "            return None\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    return \"\".join(mask)\n",
    "\n",
    "assert compute_mask(\"E506EC152\") == \"LDDDLLDDD\"\n",
    "assert compute_mask(\"E123KX99\") == \"LDDDLLDD\"\n",
    "assert compute_mask(\"P@@@KA@@\") is None\n",
    "\n",
    "def check_in_alphabet(text, alphabet=abc):\n",
    "    \"\"\"Check if all chars in text come from alphabet.\n",
    "    \n",
    "    Args:\n",
    "        - text: String of text.\n",
    "        - alphabet: String of alphabet.\n",
    "        \n",
    "    Returns:\n",
    "        True if all chars in text are from alphabet and False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for char in text:\n",
    "        if char not in alphabet:\n",
    "            return False\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    return True\n",
    "\n",
    "assert check_in_alphabet(\"E506EC152\") is True\n",
    "assert check_in_alphabet(\"A123GG999\") is False\n",
    "\n",
    "def filter_data(config):\n",
    "    \"\"\"Filter config items keeping only ones with correct text.\n",
    "    \n",
    "    Args:\n",
    "        - config: List of dicts, each dict having keys \"file\" and \"text\".\n",
    "        \n",
    "    Returns:\n",
    "        Filtered list (config subset).\n",
    "    \"\"\"\n",
    "    config_filtered = []\n",
    "    for item in tqdm.tqdm(config):\n",
    "        text = item[\"text\"]\n",
    "        mask = compute_mask(text)\n",
    "        if check_in_alphabet(text) and (mask == \"LDDDLLDD\" or mask == \"LDDDLLDDD\"):\n",
    "            config_filtered.append({\"file\": item[\"file\"],\n",
    "                                    \"text\": item[\"text\"]})\n",
    "    return config_filtered\n",
    "seminar_config = filter_data(seminar_config)\n",
    "print(\"Total items in data after filtering:\", len(seminar_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, config, alphabet=abc, transforms=None):\n",
    "        \"\"\"Constructor for class.\n",
    "        \n",
    "        Args:\n",
    "            - config: List of items, each of which is a dict with keys \"file\" & \"text\".\n",
    "            - alphabet: String of chars required for predicting.\n",
    "            - transforms: Transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        \"\"\"\n",
    "        super(RecognitionDataset, self).__init__()\n",
    "        self.config = config\n",
    "        self.alphabet = alphabet\n",
    "        self.image_names, self.texts = self._parse_root_()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def _parse_root_(self):\n",
    "        image_names, texts = [], []\n",
    "        for item in self.config:\n",
    "            image_name = item[\"file\"]\n",
    "            text = item['text']\n",
    "            texts.append(text)\n",
    "            image_names.append(image_name)\n",
    "        return image_names, texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Returns dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        Image is a numpy array, float32, [0, 1].\n",
    "        Seq is list of integers.\n",
    "        Seq_len is an integer.\n",
    "        Text is a string.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.image_names[item]).astype(np.float32) / 255.\n",
    "        text = self.texts[item]\n",
    "        seq = self.text_to_seq(text)\n",
    "        seq_len = len(seq)\n",
    "        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n",
    "        if self.transforms is not None:\n",
    "            output = self.transforms(output)\n",
    "        return output\n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        \"\"\"Encode text to sequence of integers.\n",
    "        \n",
    "        Args:\n",
    "            - String of text.\n",
    "            \n",
    "        Returns:\n",
    "            List of integers where each number is index of corresponding characted in alphabet + 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        seq = [self.alphabet.find(c) + 1 for c in text]\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, item):\n",
    "        \"\"\"Apply resizing.\n",
    "        \n",
    "        Args: \n",
    "            - item: Dict with keys \"image\", \"seq\", \"seq_len\", \"text\".\n",
    "        \n",
    "        Returns: \n",
    "            Dict with image resized to self.size.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        interpolation = cv2.INTER_AREA if self.size[0] < item[\"image\"].shape[1] else cv2.INTER_LINEAR\n",
    "        item[\"image\"] = cv2.resize(item[\"image\"], self.size, interpolation=interpolation)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation:\n",
    "    def __init__(self, max_angle, prob):\n",
    "        self.max_angle = max_angle\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, item):\n",
    "        if np.random.random() < self.prob:\n",
    "            angle = (np.random.random() * 2 - 1) * self.max_angle\n",
    "\n",
    "            (h, w) = item['image'].shape[:2]\n",
    "            (cX, cY) = (w // 2, h // 2)\n",
    "            # rotate our image by 45 degrees around the center of the image\n",
    "            M = cv2.getRotationMatrix2D((cX, cY), 45, 1.0)\n",
    "            item['image'] = cv2.warpAffine(item['image'], M, (w, h))\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    \n",
    "    Args:\n",
    "        - batch: List of dataset __getitem__ return values (dicts).\n",
    "        \n",
    "    Returns:\n",
    "        Dict with same keys but values are either torch.Tensors of batched images or sequences or so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for item in batch:\n",
    "        images.append(torch.from_numpy(item[\"image\"]).permute(2, 0, 1).float())\n",
    "        seqs.extend(item[\"seq\"])\n",
    "        seq_lens.append(item[\"seq_len\"])\n",
    "        texts.append(item[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_string(pred, abc):\n",
    "    seq = []\n",
    "    for i in range(len(pred)):\n",
    "        label = np.argmax(pred[i])\n",
    "        seq.append(label - 1)\n",
    "    out = []\n",
    "    for i in range(len(seq)):\n",
    "        if len(out) == 0:\n",
    "            if seq[i] != -1:\n",
    "                out.append(seq[i])\n",
    "        else:\n",
    "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                out.append(seq[i])\n",
    "    out = ''.join([abc[c] for c in out])\n",
    "    return out\n",
    "\n",
    "def decode(pred, abc):\n",
    "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "    outputs = []\n",
    "    for i in range(len(pred)):\n",
    "        outputs.append(pred_to_string(pred[i], abc))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path_folder, name_as_text=False):\n",
    "    config = []\n",
    "    assert os.path.isdir(path_folder)\n",
    "\n",
    "    for path in glob(os.path.join(path_folder, '*.png')):\n",
    "        if not name_as_text:\n",
    "            text = path.split('_')[-1][:-4]\n",
    "        else:\n",
    "            text = path.split('/')[-1].split('.')[0]\n",
    "\n",
    "        config.append({'file': path, 'text': text})\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_config = read_config('kaggle/train/train/simple/')\n",
    "complex_config = read_config('kaggle/train/train/complex/')\n",
    "test_config = read_config('kaggle/test/result/', name_as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, filename):\n",
    "\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        torch.save(model.state_dict(), fp)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, filename):\n",
    "\n",
    "    with open(filename, \"rb\") as fp:\n",
    "        state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание задачи\n",
    "\n",
    "Разработать каждую из компонент и объединить их в модель `Transformer`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель трансформер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Механизм внимания на несколько голов (Multi-Head Attention)\n",
    "<img src=\"figures/multi-head-attention.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        # YOUR CODE HERE\n",
    "        self.d_model   = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k       = d_model // num_heads\n",
    "        \n",
    "        self.W_q = Linear(in_features=d_model, out_features=self.d_model, bias=False)\n",
    "        self.W_k = Linear(in_features=d_model, out_features=self.d_model, bias=False)\n",
    "        self.W_v = Linear(in_features=d_model, out_features=self.d_model, bias=False)\n",
    "        self.W_o = Linear(in_features=d_model, out_features=self.d_model, bias=False)\n",
    "\n",
    "        self.scale = self.d_k ** -0.5\n",
    "                \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # YOUR CODE HERE\n",
    "        attn_scores = (Q.transpose(2, 3) @ K) * self.scale\n",
    "        if mask is not None:\n",
    "           attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = softmax(attn_scores, dim=-1)\n",
    "        output     = V @ attn_probs\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # YOUR CODE HERE\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output      = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 3\n",
    "seq_length = 11\n",
    "d_model = 10\n",
    "num_heads = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "x = torch.rand(batch_size, seq_length, d_model)\n",
    "\n",
    "mha(x, x, x).shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks\n",
    "Состоит из двух линеных слоев, которые применяется к последнему измерению, то есть для каждой позиции в последовательности используются одни и те же линейные слои, так называемые `position-wise`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.fc1  = Linear(d_model, d_ff)\n",
    "        self.fc2  = Linear(d_ff, d_model)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Позиционное кодирование (Positional Encoding)\n",
    "\n",
    "Так как в архитектуре трансформер обработка последовательности заменяется на обработку множества мы теряем информацию о порядке элементов последовательности. Чтобы отобразить информацию о позиции элемента в исходной последовательности мы используем позиционное кодирование.\n",
    "\n",
    "$$\n",
    "p(i,s)=\n",
    "\\begin{cases}\n",
    "\\sin \\Big(i*10000 \\dfrac{-2k}{d_{model}}\\Big), s = 2k + 0\\\\\n",
    "\\cos \\Big(i*10000 \\dfrac{-2k}{d_{model}}\\Big), s = 2k + 1\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "max_len = 11\n",
    "d_model = 5\n",
    "seq_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "arange = torch.arange(max_len * d_model).view(d_model, max_len)\n",
    "\n",
    "i = arange % max_len\n",
    "k = arange // max_len\n",
    "\n",
    "pe = torch.zeros(max_len, d_model)\n",
    "pe += (torch.sin(i * 10_000 * (-2) * k / d_model) * (k % 2)).T\n",
    "pe += (torch.cos(i * 10_000 * (-2) * k / d_model) * ((k + 1) % 2)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(batch_size, seq_length, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        self.scale   = d_model\n",
    "    \n",
    "        arange = torch.arange(max_len * d_model).view(d_model, max_len)\n",
    "\n",
    "        i = arange % max_len\n",
    "        k = arange // max_len\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe += (torch.sin(i * 10_000 * (-2) * k / d_model) * (k % 2)).T\n",
    "        pe += (torch.cos(i * 10_000 * (-2) * k / d_model) * ((k + 1) % 2)).T\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        self.pe = pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[: x.shape[1]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слой кодировщика (Encoder Layer)\n",
    "\n",
    "<img src=\"figures/encoder-layer.jpg\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.self_attn    = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "        self.norm1 = LayerNorm()\n",
    "        self.norm2 = LayerNorm()\n",
    "        self.dropout = Dropout(dropout)\n",
    "                \n",
    "    def forward(self, x, mask):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        attn = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(attn + x)\n",
    "\n",
    "        ff = self.feed_forward(x)\n",
    "        x = self.norm2(ff + x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слой декодировщика (Decoder Layer)\n",
    "\n",
    "<img src=\"figures/decoder-layer.jpg\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.self_attn    = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.cross_attn   = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "        self.norm1 = LayerNorm()\n",
    "        self.norm2 = LayerNorm()\n",
    "        self.norm3 = LayerNorm()\n",
    "        self.dropout = Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        \n",
    "        attn = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(attn + x)\n",
    "\n",
    "        attn = self.cross_attn(enc_output, enc_output, x, src_mask)\n",
    "\n",
    "        ff = self.feed_forward(x)\n",
    "        x = self.norm3(ff + x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель трансформер\n",
    "\n",
    "<img src=\"figures/transformer-model.jpg\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Module):\n",
    "    def __init__(self, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, dropout=dropout, max_len=max_seq_length)\n",
    "        self.encoder_layers = [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in num_layers]\n",
    "        self.decoder_layers = [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in num_layers]\n",
    "        self.fc = Linear(d_model, d_model)\n",
    "        self.dropout = Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src_embeddings, tgt_embeddings):\n",
    "        src_mask, tgt_mask = self.generate_mask(src_embeddings, tgt_embeddings)\n",
    "        encoder = self.positional_encoding(src_embeddings)\n",
    "        decoder = self.positional_encoding(tgt_embeddings)\n",
    "\n",
    "        for layer in self.encoder_layers:\n",
    "            encoder = layer(encoder, src_mask)\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            decoder = layer(decoder, encoder, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(decoder)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание задачи\n",
    "\n",
    "Мы решили задачу распознавания текста на изображениях, распознавания регистрационных знаков. Воспользовались достаточно сильной сверточной моделью для извлечения признаков `(ResNet18)` и рекуррентной моделью для обработки последовательности `(RNN,LSTM,GRU)`. Модель была достаточно простой.\n",
    "\n",
    "Попробуем воспользоваться текущими представлениями о нейронных сетях и разработаем модель, которая будет состоять также из сверточной модели, однако заменим рекуррентную модель и будем использовать архитектуру типа `Transformer`. Мы воспользуемся некоторой вариацией современного подхода к распознаванию текста на изображениях [TrOCR](https://huggingface.co/docs/transformers/model_doc/trocr)\n",
    "\n",
    "Отметим, что мы используем трансформер в режиме автогрегрессии, то с чем вы сталкивались на семинаре по рекуррентным нейронным сетям, когда разрабатывали модель, которая генерирует символы или слова. Поэтому результирующую строку мы будем `генерировать` итеративно. Как итог наша модель имеет следующий вид.\n",
    "\n",
    "![picture](figures/ModelArchitecture.png) \n",
    "\n",
    "Задача ваша состоит в том чтобы натренировать модель с использованием `nn.Transformer` и собственной реализации, получить близкие результаты. Также требуется сравнить с предыдущим подходом, где использовались рекуррентные нейронные сети. За первую часть дается 10 баллов, за вторую оставшиесь 10 баллов. За красивое оформление и графики можно получить бонусные 5 баллов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path_folder, name_as_text=False):\n",
    "    config = []\n",
    "    assert os.path.isdir(path_folder)\n",
    "\n",
    "    for path in glob(os.path.join(path_folder, '*.png')):\n",
    "        if not name_as_text:\n",
    "            text = path.split('_')[-1][:-4]\n",
    "        else:\n",
    "            text = path.split('/')[-1].split('.')[0]\n",
    "\n",
    "        config.append({'file': path, 'text': text})\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_config = read_config('kaggle/train/train/simple/')\n",
    "complex_config = read_config('kaggle/train/train/complex/')\n",
    "test_config = read_config('kaggle/test/result/', name_as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn    import Conv2d, MaxPool2d, BatchNorm2d, LeakyReLU\n",
    "from torchvision import transforms\n",
    "from collections import Counter\n",
    "from time        import time\n",
    "from torch       import nn\n",
    "from PIL         import Image\n",
    "from tqdm        import tqdm\n",
    "\n",
    "from rapidfuzz.distance.Levenshtein import distance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import string\n",
    "import torch\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR               = './' # work directory\n",
    "PATH_TRAIN_DIR    =  'kaggle/train'\n",
    "PATH_TEST_DIR     =  'kaggle/test/result/'\n",
    "PREDICT_PATH      =  'kaggle/test/result'\n",
    "CHECKPOINT_PATH   = DIR\n",
    "WEIGHTS_PATH      = ''\n",
    "PATH_TEST_RESULTS = DIR+'/test_result.tsv'\n",
    "TRAIN_LOG = DIR+'train_log.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL ### \n",
    "MODEL = 'model'\n",
    "HIDDEN     = 512\n",
    "ENC_LAYERS = 2\n",
    "DEC_LAYERS = 2\n",
    "N_HEADS    = 4\n",
    "LENGTH     = 15\n",
    "\n",
    "ALPHABET = ['PAD', 'SOS', '0','1','2','3','4','5','6','7','8','9','A','B','E','K','M','H','O','P','C','T','Y','X', 'EOS']\n",
    "\n",
    "### TRAINING ###\n",
    "BATCH_SIZE = 16\n",
    "DROPOUT    = 0.2\n",
    "N_EPOCHS   = 16\n",
    "CHECKPOINT_FREQ = 10 # save checkpoint every 10 epochs\n",
    "DEVICE = 'cuda'\n",
    "SCHUDULER_ON = True # \"ReduceLROnPlateau\"\n",
    "PATIENCE = 5 # for ReduceLROnPlateau\n",
    "OPTIMIZER_NAME = 'Adam' # or \"SGD\"\n",
    "#LR = 2e-6\n",
    "LR = 2e-5\n",
    "\n",
    "### INPUT IMAGE PARAMETERS ###\n",
    "WIDTH, HEIGHT, CHANNELS = 320, 64, 3 \n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed           (RANDOM_SEED)\n",
    "torch.manual_seed     (RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def log_metrics(metrics):\n",
    "    if metrics['epoch'] == 0:\n",
    "        print('Epoch   Train_loss   Valid_loss   LEV  Time    LR')\n",
    "        print('-----   -----------  ----------   ---  ----    ---')\n",
    "    print('{:02d}    {:.3f}       {:.3f}       {:.3f}   {:.3f}   {:.7f}'.format(\\\n",
    "        metrics['epoch'], metrics['train_loss'], metrics['eval_loss'], metrics['levenstein'], metrics['time'], metrics['lr']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузчик данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicies_to_text(indexes, idx2char):\n",
    "    text = \"\".join([idx2char[i] for i in indexes])\n",
    "    text = text.replace('EOS', '').replace('PAD', '').replace('SOS', '')\n",
    "    return text\n",
    "\n",
    "char2idx = {char: idx for idx, char in enumerate(ALPHABET)}\n",
    "idx2char = {idx: char for idx, char in enumerate(ALPHABET)}\n",
    "\n",
    "class RecognitionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform):\n",
    "        self.root_dir   = root_dir\n",
    "        self.transform  = transform\n",
    "        self.total_imgs = os.listdir(root_dir)\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.root_dir, self.total_imgs[idx])\n",
    "        f = os.path.splitext( os.path.basename( img_loc ) )[0]\n",
    "        text  = ''.join( f.split('_')[ 2: ] )\n",
    "         \n",
    "        pil_image = Image.open(img_loc).convert(\"RGB\")\n",
    "        image_tensor = self.transform(pil_image)\n",
    "        index_tensor = torch.LongTensor( [char2idx['SOS']] + [char2idx[i] for i in text if i in char2idx.keys()] + [char2idx['EOS']] )\n",
    "        \n",
    "        return (image_tensor, index_tensor)\n",
    "\n",
    "class DataCollate():\n",
    "    def __call__(self, batch):\n",
    "        image_tensor_s, index_tensor_s = [], []\n",
    "        \n",
    "        for i in range(len(batch)):\n",
    "            image_tensor = batch[i][0]\n",
    "            index_tensor = batch[i][1]\n",
    "\n",
    "            cur_len = index_tensor.shape[0]\n",
    "            \n",
    "            padding_tensor = torch.full((LENGTH,), fill_value=char2idx['PAD']).long()\n",
    "            padding_tensor[:index_tensor.shape[0]] = index_tensor\n",
    "            \n",
    "            image_tensor_s.append(image_tensor  )\n",
    "            index_tensor_s.append(padding_tensor)\n",
    "            \n",
    "        image_tensor_s = torch.stack(image_tensor_s,)\n",
    "        index_tensor_s = torch.stack(index_tensor_s,)\n",
    "               \n",
    "        return image_tensor_s, index_tensor_s.T\n",
    "                                                                                                  \n",
    "                                                                                                  \n",
    "train_trans = transforms.Compose( [\n",
    " transforms.Resize( (64, 320) ),\n",
    " transforms.ToTensor(),\n",
    " transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "] )\n",
    "\n",
    "test_trans = transforms.Compose( [\n",
    "    transforms.Resize( (64, 320) ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "] )\n",
    "\n",
    "infer_trans = transforms.Compose( [\n",
    "    transforms.Resize( (64, 320) ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RecognitionDataset(PATH_TRAIN_DIR, train_trans)\n",
    "test_dataset  = RecognitionDataset(PATH_TEST_DIR , test_trans)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True , batch_size=BATCH_SIZE, pin_memory=True, drop_last=True , collate_fn=DataCollate())\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset , shuffle=False, batch_size=BATCH_SIZE, pin_memory=True, drop_last=True, collate_fn=DataCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24546, 10518)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наша модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet34_Weights\n",
    "from torchvision        import models\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1, pretrained=True):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.enc_layers = enc_layers\n",
    "        self.dec_layers = dec_layers\n",
    "        \n",
    "        self.backbone_name = 'resnet34'\n",
    "        self.backbone = models.resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "        self.backbone.fc = nn.Conv2d(512, int(hidden/2), 1)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
    "        self.decoder     = nn.Embedding(outtoken, hidden)\n",
    "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
    "        self.transformer = nn.Transformer(\n",
    "         d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n",
    "         num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout,\n",
    "         activation='relu'\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden, outtoken)\n",
    "        self.src_mask = None\n",
    "        self.trg_mask = None\n",
    "        self.memory_mask = None\n",
    "            \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=DEVICE), 1)\n",
    "        mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def make_len_mask(self, inp):\n",
    "        return (inp == 0).transpose(0, 1)\n",
    "    \n",
    "    # it vectorizes images\n",
    "    def _get_features(self, src):\n",
    "        '''\n",
    "        params\n",
    "        ---\n",
    "        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
    "            B - batch, C - channel, H - height, W - width\n",
    "\n",
    "        returns\n",
    "        ---\n",
    "        x : Tensor : [W,B,CH]\n",
    "        '''\n",
    "        x = self.backbone.conv1(src)\n",
    "\n",
    "        x = self.backbone.bn1    (x)\n",
    "        x = self.backbone.relu   (x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "        x = self.backbone.layer1 (x)\n",
    "        x = self.backbone.layer2 (x)\n",
    "        x = self.backbone.layer3 (x)\n",
    "        x = self.backbone.layer4 (x) # [64, 2048, 2, 8] : [B,C,H,W]\n",
    "            \n",
    "        x = self.backbone.fc(x) # [64, 256, 2, 8] : [B,C,H,W]\n",
    "        x = x.permute(0, 3, 1, 2) # [64, 8, 256, 2] : [B,W,C,H]\n",
    "        x = x.flatten(2) # [64, 8, 512] : [B,W,CH]\n",
    "        x = x.permute(1, 0, 2) # [8, 64, 512] : [W,B,CH]\n",
    "        return x\n",
    "\n",
    "    def predict(self, batch):\n",
    "        '''\n",
    "        params\n",
    "        ---\n",
    "        batch : Tensor [64, 3, 64, 256] : [B,C,H,W]\n",
    "            B - batch, C - channel, H - height, W - width\n",
    "        \n",
    "        returns\n",
    "        ---\n",
    "        result : List [64, -1] : [B, -1]\n",
    "            preticted sequences of tokens' indexes\n",
    "        '''\n",
    "        result = []\n",
    "        for item in batch:\n",
    "          x = self._get_features(item.unsqueeze(0))\n",
    "          memory = self.transformer.encoder(self.pos_encoder(x))\n",
    "          out_indexes = [ALPHABET.index('SOS'), ]\n",
    "          for i in range(LENGTH):\n",
    "              trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(DEVICE)\n",
    "              output = self.fc_out(self.transformer.decoder(self.pos_decoder(self.decoder(trg_tensor)), memory))\n",
    "\n",
    "              out_token = output.argmax(2)[-1].item()\n",
    "              out_indexes.append(out_token)\n",
    "              if out_token == ALPHABET.index('EOS'):\n",
    "                  break\n",
    "          result.append(out_indexes)\n",
    "        return result\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        '''\n",
    "        params\n",
    "        ---\n",
    "        src : Tensor : [B,C,H,W]\n",
    "            B - batch, C - channel, H - height, W - width\n",
    "        trg : Tensor : [L,B]\n",
    "            L - max length of label, B - batch\n",
    "        '''\n",
    "        x = self._get_features(src)\n",
    "        src_pad_mask = self.make_len_mask(x[:, :, 0])\n",
    "        src = self.pos_encoder(x) # [8, 64, 512]\n",
    "\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n",
    "        trg_pad_mask = self.make_len_mask(trg)\n",
    "        trg = self.decoder(trg)\n",
    "        trg = self.pos_decoder(trg)\n",
    "\n",
    "        output = self.transformer(\n",
    "         src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n",
    "         memory_mask=self.memory_mask,\n",
    "         src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n",
    "         memory_key_padding_mask=src_pad_mask\n",
    "        ) # [13, 64, 512] : [L,B,CH]\n",
    "        \n",
    "        logits = self.fc_out(output) # [13, 64, 92] : [L,B,H]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, optimizer, criterion, train_loader):\n",
    "    \"\"\"\n",
    "    params\n",
    "    ---\n",
    "    model : nn.Module\n",
    "    optimizer : nn.Object\n",
    "    criterion : nn.Object\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "    returns\n",
    "    ---\n",
    "    epoch_loss / len(train_loader) : float\n",
    "        overall loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in train_loader:\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        output = model(src, trg[:-1, :])\n",
    "\n",
    "        loss   = criterion(output.view(-1, output.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss     .backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, criterion, loader, case=True, punct=True):\n",
    "    result  = {'true': [], 'pred': []}\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (src, trg) in loader:\n",
    "            src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "            logits = model(src, trg[:-1, :])\n",
    "            loss   = criterion(logits.view(-1, logits.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n",
    "            out_indexes = model.predict(src)\n",
    "\n",
    "            true_phrases = [indicies_to_text(trg[1:,i]     , ALPHABET) for i in range(BATCH_SIZE)]\n",
    "            pred_phrases = [indicies_to_text(out_indexes[i], ALPHABET) for i in range(BATCH_SIZE)]\n",
    "\n",
    "            for i in range(len(true_phrases)):\n",
    "                result['true'].append(true_phrases[i])\n",
    "                result['pred'].append(pred_phrases[i])\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(loader), result\n",
    "\n",
    "def fit(model, optimizer, scheduler, criterion, train_loader, val_loader):\n",
    "    metrics = []\n",
    "    for epoch in tqdm(range(0, N_EPOCHS)):\n",
    "        epoch_metrics = {'epoch': 0, 'lr': 0, 'levenstein': 0}\n",
    "        \n",
    "        start_time = time()\n",
    "        train_loss   = train   (model, optimizer, criterion, train_loader)\n",
    "        eval_loss, result_phrases = evaluate(model, criterion, val_loader)\n",
    "        end_time   = time()\n",
    "\n",
    "        levenstein_distance, counter = 0, 0\n",
    "        for true_phrase, pred_phrase in zip(result_phrases['true'], result_phrases['pred']):\n",
    "            levenstein_distance += distance(pred_phrase, true_phrase)\n",
    "            counter += 1\n",
    "        \n",
    "        epoch_metrics['levenstein'] = levenstein_distance / counter                 \n",
    "        epoch_metrics['train_loss'] = train_loss\n",
    "        epoch_metrics['eval_loss' ] = eval_loss\n",
    "        epoch_metrics['epoch'     ] = epoch\n",
    "        epoch_metrics['lr'        ] = optimizer.param_groups[0][\"lr\"]\n",
    "        epoch_metrics['time'      ] = end_time - start_time\n",
    "        \n",
    "        metrics.append(epoch_metrics)\n",
    "        log_metrics(epoch_metrics)\n",
    "        \n",
    "        scheduler.step(eval_loss)\n",
    "    return metrics\n",
    "model = TransformerModel(len(ALPHABET), hidden=HIDDEN, enc_layers=ENC_LAYERS, dec_layers=DEC_LAYERS, nhead=N_HEADS, dropout=DROPOUT).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=char2idx['PAD'])\n",
    "optimizer = torch.optim.__getattribute__(OPTIMIZER_NAME)(model.parameters(), lr=LR)\n",
    "\n",
    "scheduler =torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=PATIENCE)\n",
    "\n",
    "fit(model, optimizer, scheduler, criterion, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, result = evaluate(model, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "for batch in test_loader:\n",
    "    prediction.append(model.predict(batch[0].to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы\n",
    "\n",
    "К сожалению я запустил на testloader без проавильных ответов.  \n",
    "Поэтому отследить валидационное качество не удалось.  \n",
    "Но если сравнить loss на трейне с предыдущей домашкой на resnet18, то трансформеру удалось достичь более высокого качество сравнимого с рекурентной моделью, но с resnet34 в качестве бэкбона.  \n",
    "Так я сделал late_submission в kaggle и качество оказалось выше чем моя лучшая рекурентная модель.  \n",
    "Стоит отметить правда что это улучшение совершенно не бесплатное и время обучения увеличилось примерно в 4 раза.  \n",
    "Это увеличение времени обучения довольно критическое и не позволяет тестировать разные параметры так быстро, как это получалось с рекурентными моделями.  \n",
    "Судя по графику наверняка лосса, складывается ощущение что первые итерации были с неправильным lr, возможно если аккуратно подобрать lr и sheduler можно достичь гораздо более быстрого обучение (что как раз получилось сделать с рекурентной сетью в прошлой домашке).  \n",
    "В целом трансформеры однозначно очень мощная архитектура и показала более высокие результаты чем рекурентная сеть, не смотря на более слабый бэкбон и практическо 0 экспрериментов с параметрами.  \n",
    "\n",
    "UPD:\n",
    "Я заметил что в трансформере тоже resnet34, так что бэкбон у него не более слабый"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
