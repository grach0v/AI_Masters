{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Отчет](https://api.wandb.ai/links/cowboy_bebop/rfspqu29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Быстрое повторение семинара / модификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/miniconda3/envs/ml3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/denis/miniconda3/envs/ml3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from torch.nn            import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\n",
    "from torch.utils.data    import Dataset, DataLoader\n",
    "from torch.nn.functional import ctc_loss, log_softmax\n",
    "from torchvision         import models\n",
    "from torchvision import transforms as T\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from string import digits, ascii_uppercase\n",
    "\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import gc\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./seminar/data/seminar_crnn_data/\"  # Change to your path with unzipped data\n",
    "config_path = os.path.join(PATH_TO_DATA, \"config.json\")\n",
    "images_path = os.path.join(PATH_TO_DATA, \"images\")\n",
    "\n",
    "assert os.path.isfile(config_path)\n",
    "assert os.path.isdir(images_path)\n",
    "\n",
    "with open(config_path, \"rt\") as fp:\n",
    "    config = json.load(fp)\n",
    "\n",
    "config_full_paths = []\n",
    "for item in config:\n",
    "    config_full_paths.append({\"file\": os.path.join(images_path, item[\"file\"]),\n",
    "                              \"text\": item[\"text\"]})\n",
    "seminar_config = config_full_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = \"0123456789ABEKMHOPCTYX\"  # this is our alphabet for predictions.\n",
    "def compute_mask(text):\n",
    "    \"\"\"Compute letter-digit mask of text, e.g. 'E506EC152' -> 'LDDDLLDDD'.\n",
    "    \n",
    "    Args:\n",
    "        - text: String of text. \n",
    "        \n",
    "    Returns:\n",
    "        String of the same length but with every letter replaced by 'L' and every digit replaced by 'D' \n",
    "        or None if non-letter and non-digit character met in text.\n",
    "    \"\"\"\n",
    "    mask = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for char in text:\n",
    "        if char in digits:\n",
    "            mask.append(\"D\")\n",
    "        elif char in ascii_uppercase:\n",
    "            mask.append(\"L\")\n",
    "        else:\n",
    "            return None\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    return \"\".join(mask)\n",
    "\n",
    "assert compute_mask(\"E506EC152\") == \"LDDDLLDDD\"\n",
    "assert compute_mask(\"E123KX99\") == \"LDDDLLDD\"\n",
    "assert compute_mask(\"P@@@KA@@\") is None\n",
    "\n",
    "def check_in_alphabet(text, alphabet=abc):\n",
    "    \"\"\"Check if all chars in text come from alphabet.\n",
    "    \n",
    "    Args:\n",
    "        - text: String of text.\n",
    "        - alphabet: String of alphabet.\n",
    "        \n",
    "    Returns:\n",
    "        True if all chars in text are from alphabet and False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for char in text:\n",
    "        if char not in alphabet:\n",
    "            return False\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    return True\n",
    "\n",
    "assert check_in_alphabet(\"E506EC152\") is True\n",
    "assert check_in_alphabet(\"A123GG999\") is False\n",
    "\n",
    "def filter_data(config):\n",
    "    \"\"\"Filter config items keeping only ones with correct text.\n",
    "    \n",
    "    Args:\n",
    "        - config: List of dicts, each dict having keys \"file\" and \"text\".\n",
    "        \n",
    "    Returns:\n",
    "        Filtered list (config subset).\n",
    "    \"\"\"\n",
    "    config_filtered = []\n",
    "    for item in tqdm.tqdm(config):\n",
    "        text = item[\"text\"]\n",
    "        mask = compute_mask(text)\n",
    "        if check_in_alphabet(text) and (mask == \"LDDDLLDD\" or mask == \"LDDDLLDDD\"):\n",
    "            config_filtered.append({\"file\": item[\"file\"],\n",
    "                                    \"text\": item[\"text\"]})\n",
    "    return config_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41141/41141 [00:00<00:00, 730716.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in data after filtering: 31345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seminar_config = filter_data(seminar_config)\n",
    "print(\"Total items in data after filtering:\", len(seminar_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionDataset(Dataset):\n",
    "    \"\"\"Class for training image-to-text mapping using CTC-Loss.\"\"\"\n",
    "\n",
    "    def __init__(self, config, alphabet=abc, transforms=None):\n",
    "        \"\"\"Constructor for class.\n",
    "        \n",
    "        Args:\n",
    "            - config: List of items, each of which is a dict with keys \"file\" & \"text\".\n",
    "            - alphabet: String of chars required for predicting.\n",
    "            - transforms: Transformation for items, should accept and return dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        \"\"\"\n",
    "        super(RecognitionDataset, self).__init__()\n",
    "        self.config = config\n",
    "        self.alphabet = alphabet\n",
    "        self.image_names, self.texts = self._parse_root_()\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def _parse_root_(self):\n",
    "        image_names, texts = [], []\n",
    "        for item in self.config:\n",
    "            image_name = item[\"file\"]\n",
    "            text = item['text']\n",
    "            texts.append(text)\n",
    "            image_names.append(image_name)\n",
    "        return image_names, texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Returns dict with keys \"image\", \"seq\", \"seq_len\" & \"text\".\n",
    "        Image is a numpy array, float32, [0, 1].\n",
    "        Seq is list of integers.\n",
    "        Seq_len is an integer.\n",
    "        Text is a string.\n",
    "        \"\"\"\n",
    "        image = cv2.imread(self.image_names[item]).astype(np.float32) / 255.\n",
    "        text = self.texts[item]\n",
    "        seq = self.text_to_seq(text)\n",
    "        seq_len = len(seq)\n",
    "        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n",
    "        if self.transforms is not None:\n",
    "            output = self.transforms(output)\n",
    "        return output\n",
    "\n",
    "    def text_to_seq(self, text):\n",
    "        \"\"\"Encode text to sequence of integers.\n",
    "        \n",
    "        Args:\n",
    "            - String of text.\n",
    "            \n",
    "        Returns:\n",
    "            List of integers where each number is index of corresponding characted in alphabet + 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        seq = [self.alphabet.find(c) + 1 for c in text]\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "\n",
    "    def __init__(self, size=(320, 64)):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, item):\n",
    "        \"\"\"Apply resizing.\n",
    "        \n",
    "        Args: \n",
    "            - item: Dict with keys \"image\", \"seq\", \"seq_len\", \"text\".\n",
    "        \n",
    "        Returns: \n",
    "            Dict with image resized to self.size.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        interpolation = cv2.INTER_AREA if self.size[0] < item[\"image\"].shape[1] else cv2.INTER_LINEAR\n",
    "        item[\"image\"] = cv2.resize(item[\"image\"], self.size, interpolation=interpolation)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation:\n",
    "    def __init__(self, max_angle, prob):\n",
    "        self.max_angle = max_angle\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, item):\n",
    "        if np.random.random() < self.prob:\n",
    "            angle = (np.random.random() * 2 - 1) * self.max_angle\n",
    "\n",
    "            (h, w) = item['image'].shape[:2]\n",
    "            (cX, cY) = (w // 2, h // 2)\n",
    "            # rotate our image by 45 degrees around the center of the image\n",
    "            M = cv2.getRotationMatrix2D((cX, cY), 45, 1.0)\n",
    "            item['image'] = cv2.warpAffine(item['image'], M, (w, h))\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, item):\n",
    "        item['image'] = (item['image'] - self.mean) / self.std\n",
    "        return item        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Function for torch.utils.data.Dataloader for batch collecting.\n",
    "    \n",
    "    Args:\n",
    "        - batch: List of dataset __getitem__ return values (dicts).\n",
    "        \n",
    "    Returns:\n",
    "        Dict with same keys but values are either torch.Tensors of batched images or sequences or so.\n",
    "    \"\"\"\n",
    "    images, seqs, seq_lens, texts = [], [], [], []\n",
    "    for item in batch:\n",
    "        images.append(torch.from_numpy(item[\"image\"]).permute(2, 0, 1).float())\n",
    "        seqs.extend(item[\"seq\"])\n",
    "        seq_lens.append(item[\"seq_len\"])\n",
    "        texts.append(item[\"text\"])\n",
    "    images = torch.stack(images)\n",
    "    seqs = torch.Tensor(seqs).int()\n",
    "    seq_lens = torch.Tensor(seq_lens).int()\n",
    "    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(Module):\n",
    "    \n",
    "    def __init__(self, input_size=(64, 320), output_len=20, model='resnet18'):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        \n",
    "        h, w = input_size\n",
    "        resnet = getattr(models, model)(pretrained=True)\n",
    "        self.cnn = Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        self.pool = AvgPool2d(kernel_size=(h // 32, 1))        \n",
    "        self.proj = Conv2d(w // 32, output_len, kernel_size=1)\n",
    "  \n",
    "        self.num_output_features = self.cnn[-1][-1].bn2.num_features    \n",
    "    \n",
    "    def apply_projection(self, x):\n",
    "        \"\"\"Use convolution to increase width of a features.\n",
    "        \n",
    "        Args:\n",
    "            - x: Tensor of features (shaped B x C x H x W).\n",
    "            \n",
    "        Returns:\n",
    "            New tensor of features (shaped B x C x H x W').\n",
    "        \"\"\"\n",
    "        x = x.permute(0, 3, 2, 1).contiguous()\n",
    "        x = self.proj(x)\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        \n",
    "        return x\n",
    "   \n",
    "    def forward(self, x):\n",
    "        # Apply conv layers\n",
    "        features = self.cnn(x)\n",
    "        \n",
    "        # Pool to make height == 1\n",
    "        features = self.pool(features)\n",
    "        \n",
    "        # Apply projection to increase width\n",
    "        features = self.apply_projection(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denis/miniconda3/envs/ml3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/denis/miniconda3/envs/ml3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/denis/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [02:29<00:00, 686kB/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/denis/Documents/AI_Masters/Machine_Learning_3/HW_4/HW_4.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denis/Documents/AI_Masters/Machine_Learning_3/HW_4/HW_4.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m320\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/denis/Documents/AI_Masters/Machine_Learning_3/HW_4/HW_4.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m feature_extractor(x)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/denis/Documents/AI_Masters/Machine_Learning_3/HW_4/HW_4.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m y\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m (\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m512\u001b[39m, \u001b[39m20\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_extractor = FeatureExtractor(model='resnet50')\n",
    "x = torch.randn(1, 3, 64, 320)\n",
    "y = feature_extractor(x)\n",
    "assert y.size() == (1, 1, 512, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 2048, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencePredictor(Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3, bidirectional=False):\n",
    "        super(SequencePredictor, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes        \n",
    "        self.rnn = GRU(input_size=input_size,\n",
    "                       hidden_size=hidden_size,\n",
    "                       num_layers=num_layers,\n",
    "                       dropout=dropout,\n",
    "                       bidirectional=bidirectional)\n",
    "        \n",
    "        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n",
    "        self.fc = Linear(in_features=fc_in,\n",
    "                         out_features=num_classes)\n",
    "    \n",
    "    def _init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize new tensor of zeroes for RNN hidden state.\n",
    "        \n",
    "        Args:\n",
    "            - batch_size: Int size of batch\n",
    "            \n",
    "        Returns:\n",
    "            Tensor of zeros shaped (num_layers * num_directions, batch, hidden_size).\n",
    "        \"\"\"\n",
    "        num_directions = 2 if self.rnn.bidirectional else 1\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        h = torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return h\n",
    "        \n",
    "    def _reshape_features(self, x):\n",
    "        \"\"\"Change dimensions of x to fit RNN expected input.\n",
    "        \n",
    "        Args:\n",
    "            - x: Tensor x shaped (B x (C=1) x H x W).\n",
    "        \n",
    "        Returns:\n",
    "            New tensor shaped (W x B x H).\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self._reshape_features(x)\n",
    "        \n",
    "        batch_size = x.size(1)\n",
    "        h_0 = self._init_hidden(batch_size)\n",
    "        h_0 = h_0.to(x.device)\n",
    "        x, h = self.rnn(x, h_0)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNN(Module):\n",
    "    \n",
    "    def __init__(self, alphabet=abc,\n",
    "                 cnn_input_size=(64, 320), cnn_output_len=20, cnn_model='resnet18',\n",
    "                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.3, rnn_bidirectional=False):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.features_extractor = FeatureExtractor(\n",
    "            input_size=cnn_input_size, output_len=cnn_output_len, model=cnn_model,\n",
    "        )\n",
    "        self.sequence_predictor = SequencePredictor(\n",
    "            input_size=self.features_extractor.num_output_features,\n",
    "            hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n",
    "            num_classes=len(alphabet)+1, dropout=rnn_dropout,\n",
    "            bidirectional=rnn_bidirectional\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.features_extractor(x)\n",
    "        sequence = self.sequence_predictor(features)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_to_string(pred, abc):\n",
    "    seq = []\n",
    "    for i in range(len(pred)):\n",
    "        label = np.argmax(pred[i])\n",
    "        seq.append(label - 1)\n",
    "    out = []\n",
    "    for i in range(len(seq)):\n",
    "        if len(out) == 0:\n",
    "            if seq[i] != -1:\n",
    "                out.append(seq[i])\n",
    "        else:\n",
    "            if seq[i] != -1 and seq[i] != seq[i - 1]:\n",
    "                out.append(seq[i])\n",
    "    out = ''.join([abc[c] for c in out])\n",
    "    return out\n",
    "\n",
    "def decode(pred, abc):\n",
    "    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n",
    "    outputs = []\n",
    "    for i in range(len(pred)):\n",
    "        outputs.append(pred_to_string(pred[i], abc))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(path_folder, name_as_text=False):\n",
    "    config = []\n",
    "    assert os.path.isdir(path_folder)\n",
    "\n",
    "    for path in glob(os.path.join(path_folder, '*.png')):\n",
    "        if not name_as_text:\n",
    "            text = path.split('_')[-1][:-4]\n",
    "        else:\n",
    "            text = path.split('/')[-1].split('.')[0]\n",
    "\n",
    "        config.append({'file': path, 'text': text})\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_config = read_config('kaggle/train/train/simple/')\n",
    "complex_config = read_config('kaggle/train/train/complex/')\n",
    "test_config = read_config('kaggle/test/result/', name_as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, filename):\n",
    "\n",
    "    with open(filename, \"wb\") as fp:\n",
    "        torch.save(model.state_dict(), fp)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, filename):\n",
    "\n",
    "    with open(filename, \"rb\") as fp:\n",
    "        state_dict = torch.load(fp, map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_loaders(data_config, test_config, model_config):\n",
    "    batch_size = model_config['batch_size']\n",
    "    num_workers = model_config['num_workers']\n",
    "    train_perc = model_config['train_perc']\n",
    "\n",
    "    # Get mean and std\n",
    "    # mean = np.asarray([0.52157311, 0.5122762 , 0.50537334])\n",
    "    # std = np.asarray([0.29142887, 0.29255962, 0.29303916])\n",
    "\n",
    "    # Transforms\n",
    "    transforms_train = T.Compose([\n",
    "        Resize(),\n",
    "        # Normalize(mean, std),\n",
    "        RandomRotation(15, 0.3),\n",
    "    ])\n",
    "\n",
    "    transforms_val = T.Compose([\n",
    "        Resize(),\n",
    "        # Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "\n",
    "    np.random.shuffle(data_config)\n",
    "    train_size = int(len(data_config) * train_perc)\n",
    "    config_train = data_config[:train_size]\n",
    "    config_val   = data_config[train_size:]\n",
    "\n",
    "    train_dataset = RecognitionDataset(config_train, transforms=transforms_train)\n",
    "    val_dataset   = RecognitionDataset(config_val  , transforms=transforms_val)\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, \n",
    "                                drop_last=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, \n",
    "                                batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, \n",
    "                                drop_last=False, collate_fn=collate_fn)\n",
    "    \n",
    "\n",
    "    test_dataset = RecognitionDataset(test_config, transforms=transforms_val)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, \n",
    "        drop_last=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_model(model, train_dataloader, optimizer):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n",
    "        images = b[\"image\"].to(device)\n",
    "        seqs_gt = b[\"seq\"]\n",
    "        seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "        seqs_pred = model(images).cpu()\n",
    "        log_probs = log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "        loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                        targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                        input_lengths=seq_lens_pred,  # N\n",
    "                        target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        wandb.log({'train_loss': loss.item()})\n",
    "    \n",
    "    return model, np.mean(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, val_dataloader):\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n",
    "        images = b[\"image\"].to(device)\n",
    "        seqs_gt = b[\"seq\"]\n",
    "        seq_lens_gt = b[\"seq_len\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seqs_pred = model(images).cpu()\n",
    "            \n",
    "        log_probs = log_softmax(seqs_pred, dim=2)\n",
    "        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n",
    "\n",
    "        loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n",
    "                        targets=seqs_gt,  # N, S or sum(target_lengths)\n",
    "                        input_lengths=seq_lens_pred,  # N\n",
    "                        target_lengths=seq_lens_gt)  # N\n",
    "\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, num_epochs, optimizer, sheduler, name):\n",
    "    \n",
    "    best_val_loss = np.infty\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss = train_epoch_model(model, train_dataloader, optimizer)\n",
    "        val_loss = test_model(model, val_dataloader)\n",
    "        \n",
    "        print(f'train loss {train_loss:.4f} val loss {val_loss:.4f}')\n",
    "        wandb.log({\n",
    "            'train_loss_epoch': train_loss, \n",
    "            'val_loss_epoch': val_loss, \n",
    "            'epoch': epoch,\n",
    "            'lr': sheduler.get_last_lr()[0]\n",
    "        })\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            save_checkpoint(model, f'models/{name}_{val_loss:.4f}.pth')\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "        sheduler.step()\n",
    "\n",
    "    load_checkpoint(model, f'models/{name}_{best_val_loss:.4f}.pth')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, test_dataloader):\n",
    "    model.eval()\n",
    "    test_pred = {'index': [], 'label': []}\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        images = batch['image'].to(device)\n",
    "        \n",
    "        index = batch['text']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seqs_pred = model(images).cpu()\n",
    "            \n",
    "        texts_pred = decode(seqs_pred, model.alphabet)\n",
    "\n",
    "        test_pred['index'] += index\n",
    "        test_pred['label'] += texts_pred\n",
    "\n",
    "    return pd.DataFrame(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seminar_config, simple_config, complex_config, test_config, model_config):\n",
    "    \n",
    "    # Wandb\n",
    "    name = model_config['name']\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"hw4\",\n",
    "        name=name,\n",
    "        reinit=True,\n",
    "        config=model_config\n",
    "    )\n",
    "\n",
    "    # Data\n",
    "    all_config = seminar_config + simple_config + complex_config\n",
    "    train_dataloader, val_dataloader, test_dataloader = get_train_val_test_loaders(all_config, test_config, model_config)\n",
    "\n",
    "\n",
    "    # Create model\n",
    "    model = CRNN(\n",
    "        cnn_input_size=(model_config['cnn_height'], model_config['cnn_width']),\n",
    "        cnn_model=model_config['cnn_model'],\n",
    "        cnn_output_len=model_config['cnn_output_len'],\n",
    "        rnn_hidden_size=model_config['rnn_hidden_size'],\n",
    "        rnn_num_layers=model_config['rnn_num_layers'],\n",
    "        rnn_bidirectional=model_config['rnn_bidirectional']\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=model_config['lr'], \n",
    "        weight_decay=model_config['weight_decay']\n",
    "    )\n",
    "\n",
    "    sheduler = lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=model_config['sheduler_step'], \n",
    "        gamma=model_config['sheduler_gamma']\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model = train_model(\n",
    "        model, train_dataloader, val_dataloader, \n",
    "        model_config['num_epochs'], \n",
    "        optimizer, \n",
    "        sheduler,\n",
    "        name,\n",
    "    )\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Predict\n",
    "    predictions = make_prediction(model, test_dataloader)\n",
    "    predictions.to_csv(f'predictions/{name}.csv', index=None)\n",
    "\n",
    "    wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'name': 'Resnet50',\n",
    "    'train_perc': 0.8,\n",
    "\n",
    "    'num_epochs': 25,\n",
    "    'batch_size': 256,\n",
    "    'num_workers': 4,\n",
    "\n",
    "    'cnn_height': 64,\n",
    "    'cnn_width': 320,\n",
    "    'cnn_model': 'resnet50',\n",
    "    'cnn_output_len': 20,\n",
    "\n",
    "    'rnn_hidden_size': 128, \n",
    "    'rnn_num_layers': 2,\n",
    "    'rnn_dropout': 0.3,\n",
    "    'rnn_bidirectional': False,\n",
    "\n",
    "    'lr': 3e-4,\n",
    "    'sheduler': 'StepLR',\n",
    "    'sheduler_gamma': 0.5,\n",
    "    'sheduler_step': 8,\n",
    "    'weight_decay': 3e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_config = seminar_config + simple_config + complex_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = get_train_val_test_loaders(\n",
    "    all_config, test_config, model_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(seminar_config, simple_config, complex_config, test_config, model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'predictions/{model_config[\"name\"]}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mask'] = df['label'].apply(compute_mask)\n",
    "masks = []\n",
    "df.query('(mask != \"LDDDLLDD\") and (mask != \"LDDDLLDDD\")').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
