{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выращиваем решающее дерево своими руками\n",
    "\n",
    "### OzonMasters, \"Машинное обучение 1\"\n",
    "\n",
    "В этом ноутбуке вам предлагается реализовать решающее дерево."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала импортируем библиотеки, которые нам понадобятся в дальнейшем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решающее дерево\n",
    "\n",
    "Вспомним, что такое решающее дерево.\n",
    "\n",
    "Решающее дерево - алгоритм машинного обучения, задающийся специальным графом-деревом. В данном задании мы будем использовать бинарное дерево. Каждая внутренняя вершина такого дерева соответствует функции предикату $\\mathbb{I}[x_{\\alpha} \\geq \\beta]$. Каждая листовая вершина соответствует некоторому значению ответа, которое будет выдавать алгоритм (вещественное число в случае регрессии, номер класса или вектор вероятностей в случае классификации).\n",
    "\n",
    "На этапе обучения нам необходимо построить само дерево, а также выбрать $\\alpha$ и $\\beta$ для каждой внутренней вершины и метку прогноза для каждой листовой вершины. Задача построения \"наилучшего\" дерева (например того, которое не совершает ошибок и имеет минимальное число вершин) является NP-полной, поэтому при построении деревьев на практике приходиться использовать жадные алгоритмы.\n",
    "\n",
    "На этапе применения все объекты пропускаются через дерево. Изначально, для каждого объекта вычисляется значение функции-предиката корневой вершины. Если оно равно нулю, то алгоритм переходит в левую дочернюю вершину, иначе в правую. Затем вычисляется значение предиката в новой вершине и делается переход или влево, или вправо. Процесс продолжается, пока не будет достигнута листовая вершина. Алгоритм возвращает то значение, которое будет приписано этой вершине.\n",
    "\n",
    "### Выбор $\\alpha$ и $\\beta$\n",
    "\n",
    "На этапе построения дерева мы будем выбирать предикаты для каждой новой вершины, максимизируя функционал качества для разбиения вершины на два поддерева, который можно записать в следующем виде:\n",
    "$$ Q(R, \\alpha, \\beta) = H(R) - \\frac{|R_l|}{|R|} H(R_l) - \\frac{|R_r|}{|R|} H(R_r) $$\n",
    "\n",
    "* $H$ - критерий информативности\n",
    "* $R$ - объекты в текущей вершине\n",
    "* $R_r$ - объекты, попадающие в правое поддерево\n",
    "* $R_l$ - объекты, попадающие в левое поддерево\n",
    "\n",
    "Например, критерий информативности Джини для задачи классификации:\n",
    "\n",
    "$$ H(R) = \\sum_{k=1}^K p_k (1 - p_k) = 1 - \\sum_{k=1}^K p_k^2 $$\n",
    "\n",
    "* $p_k$ - доля объектов с классом $k$ среди $R$\n",
    "* $K$ - общее число классов\n",
    "\n",
    "При разбиении вершины на два поддерева мы хотим максимизировать функционал качества, оптимизируя:\n",
    "\n",
    "* $\\alpha$ - номер признака в предикате\n",
    "* $\\beta$ - пороговое значение предиката\n",
    "\n",
    "В данном задании оптимизацию мы будем проводить, используя полный перебор значений. Для $\\alpha$ множество перебираемых значений - все имеющиеся признаки, для $\\beta$ - все встречающиеся в обучающей выборке значения каждого признака, кроме наименьшего и наибольшего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Построение критерия и вычисление предиката"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном пункте предлагается:\n",
    "- реализовать метод `get_best_split` у абстрактного класса `Criterion`, который выполняет поиск порога разбиения $\\beta$ по вектору признаков;\n",
    "- для каждого из критериев `GiniCriterion`, `EntropyCriterion`, `MSECriterion` реализовать два метода:\n",
    "    - `score`, считает чистоту узла, т.е. непосредственно сам критерий $H(R)$;\n",
    "    - `get_predict_val`, вычисляет предсказанное значение в листе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion:\n",
    "\n",
    "    def get_best_split(self, feature, target):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature : feature vector, np.ndarray.shape = (n_samples, )\n",
    "        target  : target vector, np.ndarray.shape = (n_samples, )\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        threshold : value to split feature vector, float\n",
    "        q_value   : impurity improvement, float\n",
    "        \"\"\"\n",
    "\n",
    "        size = len(feature)\n",
    "        init_score = self.score(target)\n",
    "        sort_ids = np.argsort(feature)\n",
    "\n",
    "        best_score = -np.inf\n",
    "        best_i = None\n",
    "\n",
    "        for i in range(len(feature)):\n",
    "            left_size = i + 1\n",
    "            left_ids = sort_ids[:i + 1]\n",
    "\n",
    "            right_size = size - i - 1\n",
    "            right_ids = sort_ids[i + 1:]\n",
    "\n",
    "            left_score = left_size / size * self.score(target[left_ids])\n",
    "            right_score = right_size / size * self.score(target[right_ids])\n",
    "\n",
    "            score = init_score - left_score - right_score\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_i = i\n",
    "\n",
    "        best_threshold = (feature[sort_ids[best_i]] + feature[sort_ids[best_i + 1]]) / 2\n",
    "\n",
    "        return best_threshold, best_score\n",
    "\n",
    "\n",
    "    def score(self, target):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : target vector, np.ndarray.shape = (n_samples, )\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        impurity : float\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_predict_val(self, target):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        target : target vector, np.ndarray.shape = (n_samples, )\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        prediction :\n",
    "            - classification: probability distribution in node, np.ndarray.shape = (n_classes, )\n",
    "            - regression: best constant approximation, float\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class GiniCriterion(Criterion):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "    \n",
    "    def get_predict_val(self, classes):\n",
    "        if len(classes) == 0:\n",
    "            return np.zeros(0)\n",
    "\n",
    "        pred = np.bincount(classes, minlength = self.n_classes) / len(classes)\n",
    "        return pred\n",
    "\n",
    "    def score(self, classes):\n",
    "        pred = self.get_predict_val(classes)\n",
    "        return 1 - (pred ** 2).sum()\n",
    "        \n",
    "\n",
    "class EntropyCriterion(Criterion):\n",
    "    EPS = 1e-6\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def get_predict_val(self, classes):\n",
    "        pred = np.bincount(classes, minlength = self.n_classes) / len(classes)\n",
    "        return pred\n",
    "\n",
    "    def score(self, classes):\n",
    "        pred = self.get_predict_val(classes)\n",
    "        return (-pred*np.log(pred + self.EPS)).sum()\n",
    "\n",
    "\n",
    "class MSECriterion(Criterion):\n",
    "    def get_predict_val(self, target):\n",
    "        if len(target) == 0:\n",
    "            return 0\n",
    "        return np.mean(target)\n",
    "\n",
    "    def score(self, target):\n",
    "        if len(target) == 0:\n",
    "            return 0\n",
    "        return np.var(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверь себя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [1, 1, 2, 4, 2, 2, 0, 1, 0, 4]\n",
    "y_true = [0.2, 0.3, 0.3, 0, 0.2]\n",
    "y_pred = GiniCriterion(n_classes=5).get_predict_val(target)\n",
    "assert np.allclose(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [1, 1, 2, 4, 2, 2, 0, 1, 0, 4]\n",
    "scores = GiniCriterion(n_classes=5).score(target)\n",
    "assert np.isclose(scores, 0.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "x = np.arange(N)\n",
    "y = np.r_[np.ones(N // 2), np.zeros(N - N // 2)].astype(int)\n",
    "\n",
    "threshold, q_best = GiniCriterion(n_classes=2).get_best_split(x, y)\n",
    "assert np.isclose(threshold, 49.5)\n",
    "assert np.isclose(q_best, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вычисление параметров предиката и разбиение вершины дерева\n",
    "\n",
    "Ниже вам предлагается реализовать несколько методов. Для класса `TreeNode` необходимо реализовать методы:\n",
    "* `get_best_split` - вычисление оптимальных $\\alpha$, $\\beta$ для выбранного функционалиа $Q(R, \\alpha, \\beta)$;\n",
    "* `split` – разбиение узла с заданными подобранными параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, impurity, predict_val, depth):\n",
    "        self.impurity = impurity        # node impurity\n",
    "        self.predict_val = predict_val  # prediction of node\n",
    "        self.depth = depth              # current node depth\n",
    "        \n",
    "        self.feature = None             # feature to split\n",
    "        self.threshold = None           # threshold to split\n",
    "        self.improvement = -np.inf      # node impurity improvement after split\n",
    "        \n",
    "        self.child_left = None\n",
    "        self.child_right = None\n",
    "    \n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        return self.child_left is None and self.child_right is None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_best_split(cls, X, y, criterion):\n",
    "        '''\n",
    "        Finds best split for current node\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : samples in node, np.ndarray.shape = (n_samples, n_features)\n",
    "        y : target values, np.ndarray.shape = (n_samples, )\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        feature   : best feature to split,  int\n",
    "        threshold : value to split feature, float\n",
    "        q_value   : impurity improvement,   float\n",
    "        '''\n",
    "\n",
    "        q_best = - np.inf\n",
    "        t_best = None\n",
    "        f_best = None\n",
    "        \n",
    "        for fi in range(X.shape[1]):\n",
    "            thr, q = criterion.get_best_split(X[:,fi], y)\n",
    "            if q > q_best:\n",
    "                q_best = q\n",
    "                t_best = thr\n",
    "                f_best = fi\n",
    "    \n",
    "        return f_best, t_best, q_best\n",
    "      \n",
    "\n",
    "    def get_best_split_mask(self, X):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : samples in node, np.ndarray.shape = (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        right_mask : indicates samples in right node after split\n",
    "            np.ndarray.shape = (n_samples, )\n",
    "            np.ndarray.dtype = bool\n",
    "        '''\n",
    "\n",
    "        return X[:, self.feature] >= self.threshold\n",
    "    \n",
    "    def split(self, X, y, criterion, **split_params):\n",
    "        '''\n",
    "        Split current node\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : samples in node, np.ndarray.shape = (n_samples, n_features)\n",
    "        y : target values, np.ndarray.shape = (n_samples, )\n",
    "        criterion : criterion to split by, Criterion\n",
    "        split_params : result of get_best_split method\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        right_mask : indicates samples in right node after split\n",
    "            np.ndarray.shape = (n_samples, )\n",
    "            np.ndarray.dtype = bool\n",
    "            \n",
    "        child_left  : TreeNode\n",
    "        child_right : TreeNode\n",
    "        '''\n",
    "        \n",
    "        self.feature = split_params['feature']\n",
    "        self.threshold = split_params['threshold']\n",
    "        self.improvement = split_params['improvement']\n",
    "        \n",
    "        mask_right = self.get_best_split_mask(X)\n",
    "        self.child_left = self.from_criterion(y[~mask_right], criterion, self.depth + 1)\n",
    "        self.child_right = self.from_criterion(y[mask_right], criterion, self.depth + 1)\n",
    "        \n",
    "        return mask_right, self.child_left, self.child_right\n",
    "            \n",
    "    @classmethod\n",
    "    def from_criterion(cls, y, criterion, depth=0):\n",
    "        return cls(\n",
    "            impurity=criterion.score(y),\n",
    "            predict_val=criterion.get_predict_val(y),\n",
    "            depth=depth,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте себя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 100, 10\n",
    "f = 3\n",
    "\n",
    "criterion = GiniCriterion(n_classes=2)\n",
    "x = np.c_[[np.arange(N) if f == fi else np.random.random(size=N) for fi in range(M)]].T\n",
    "y = np.r_[np.ones(N // 2), np.zeros(N - N // 2)].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature, best_threshold, improvement = TreeNode.get_best_split(x, y, criterion)\n",
    "\n",
    "assert best_feature == f\n",
    "assert best_threshold == 49.5\n",
    "assert improvement == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация дерева.\n",
    "\n",
    "Вот мы и добрались до самого важного. В классе `DecisionTree` вам необходимо реализовать следующие методы:\n",
    "* fit - обучения дерева\n",
    "* predict - выдача предсказаний по дереву\n",
    "\n",
    "Дерево предлагается строить самым простым способом – рекурсивно. Для реализации предлагается использовать два вспомогательных метода: \n",
    "* `_build_nodes` - вспомогательный рекурсивный метод для fit, разделяет вершину на две, если не выполняются условия останова;\n",
    "* `_get_nodes_predictions` - вспомогательный рекурсивный метод для predict, пропускает объекты через вершины и заполняет матрицу предсказаний.\n",
    "\n",
    "Важный вопрос при реализации: как выбрать критерий останова создания новой вершины?\n",
    "\n",
    "Вершина не будет разветвляться, если выполнено хотя бы одно из четырех условий:\n",
    "* если вершина на глубине `max_depth`;\n",
    "* если в вершине меньше, чем `min_leaf_size` объектов;\n",
    "* если в вершине все объекты имеют одинаковые метки;\n",
    "* если функционал качества не увеличивается больше, чем на `min_improvement`.\n",
    "\n",
    "Также для интерпретации важности признаков, участвующих в построении дерева, предлагается реализовать метод-свойство `feature_importances_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_leaf_size=None, min_improvement=None):\n",
    "        self.criterion = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_leaf_size = min_leaf_size\n",
    "        self.min_improvement = min_improvement\n",
    "\n",
    "    def _build_nodes(self, X, y, criterion, indices, node):\n",
    "        '''\n",
    "        Builds tree recursively\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : samples in node, np.ndarray.shape = (n_samples, n_features)\n",
    "        y : target values, np.ndarray.shape = (n_samples, )\n",
    "        criterion : criterion to split by, Criterion\n",
    "        indices : samples' indices in node,\n",
    "            np.ndarray.shape = (n_samples, )\n",
    "            nd.ndarray.dtype = int\n",
    "        node : current node to split, TreeNode\n",
    "        '''\n",
    "        \n",
    "        if self.max_depth is not None and node.depth >= self.max_depth:\n",
    "            return\n",
    "        \n",
    "        if self.min_leaf_size is not None and self.min_leaf_size > len(indices):\n",
    "            return\n",
    "        \n",
    "        if np.unique(y[indices]).shape[0] <= 1:\n",
    "            return\n",
    "\n",
    "        X_node = X[indices]\n",
    "        y_node = y[indices]\n",
    "\n",
    "        feature, threshold, improvement = node.get_best_split(X_node, y_node, criterion)\n",
    "\n",
    "        if self.min_improvement is not None and self.min_improvement > improvement:\n",
    "            return\n",
    "\n",
    "        mask_right, child_left, child_right = node.split(\n",
    "            X_node, y_node, criterion,\n",
    "            feature=feature,\n",
    "            threshold=threshold,\n",
    "            improvement=improvement\n",
    "        )\n",
    "\n",
    "        self._build_nodes(X, y, criterion, indices[~mask_right], child_left)\n",
    "        self._build_nodes(X, y, criterion, indices[mask_right], child_right)\n",
    "\n",
    "\n",
    "    def _get_nodes_predictions(self, X, predictions, indices, node):\n",
    "        '''\n",
    "        Builds tree recursively\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : samples in node, np.ndarray.shape = (n_samples, n_features)\n",
    "        predictions : result matrix to be feild,\n",
    "            - classification : np.ndarray.shape = (n_samples, n_classes)\n",
    "            - regression : np.ndarray.shape = (n_samples, )\n",
    "        indices : samples' indices in node,\n",
    "            np.ndarray.shape = (n_samples, )\n",
    "            nd.ndarray.dtype = int\n",
    "        node : current node to split, TreeNode\n",
    "        '''\n",
    "\n",
    "        if len(indices) == 0:\n",
    "            return\n",
    "\n",
    "        if node.is_terminal:\n",
    "            predictions[indices] = node.predict_val\n",
    "            return\n",
    "\n",
    "        feature = node.feature\n",
    "        threshold = node.threshold\n",
    "        mask_right = X[indices, feature] >= threshold\n",
    "\n",
    "        indices_left = indices[~mask_right]\n",
    "        indices_right = indices[mask_right]\n",
    "\n",
    "        self._get_nodes_predictions(X, predictions, indices_left, node.child_left)\n",
    "        self._get_nodes_predictions(X, predictions, indices_right, node.child_right)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        '''\n",
    "        Returns\n",
    "        -------\n",
    "        importance : cummulative improvement per feature, np.ndarray.shape = (n_features, )\n",
    "        '''\n",
    "\n",
    "        importance = np.zeros(self.n_features_)\n",
    "        quene = deque()\n",
    "        quene.append(self.root_)\n",
    "        \n",
    "        while len(quene):\n",
    "            node = quene.popleft()\n",
    "            if node.is_terminal:\n",
    "                continue\n",
    "                \n",
    "            importance[node.feature] += node.improvement\n",
    "            quene.append(node.child_left)\n",
    "            quene.append(node.child_right)\n",
    "            \n",
    "        return importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь на основе реализованного в общем виде дерева, сделаем дерево для классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDecisionTree(DecisionTree):\n",
    "    def __init__(self, criterion='gini', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if criterion not in ('gini', 'entropy', ):\n",
    "            raise ValueError('Unsupported criterion', criterion)\n",
    "        self.criterion = criterion\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = np.max(y) + 1\n",
    "        self.n_features_ = X.shape[1]\n",
    "        \n",
    "        if self.criterion == 'gini':\n",
    "            criterion = GiniCriterion(n_classes=self.n_classes_)\n",
    "        elif self.criterion == 'entropy':\n",
    "            criterion = EntropyCriterion(n_classes=self.n_classes_)\n",
    "        else:\n",
    "            raise ValueError('Unsupported criterion', criterion)\n",
    "\n",
    "        self.root_ = TreeNode.from_criterion(y, criterion)\n",
    "        self._build_nodes(X, y, criterion, np.arange(X.shape[0]), self.root_)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probas = np.zeros(shape=(X.shape[0], self.n_classes_))\n",
    "        self._get_nodes_predictions(X, probas, np.arange(X.shape[0]), self.root_)\n",
    "        return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверь себя:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=2, min_leaf_size=1)\n",
    "\n",
    "some_X = np.vstack((\n",
    "    np.random.normal(loc=(-5, -5), size=(100, 2)),\n",
    "    np.random.normal(loc=(-5, 5), size=(100, 2)),\n",
    "    np.random.normal(loc=(5, -5), size=(100, 2)),\n",
    "    np.random.normal(loc=(5, 5), size=(100, 2)),\n",
    "))\n",
    "\n",
    "some_y = np.array(\n",
    "    [0] * 100 + [1] * 100 + [2] * 100 + [3] * 100\n",
    ")\n",
    "\n",
    "some_tree.fit(some_X, some_y)\n",
    "predictions = some_tree.predict(some_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_X.shape, some_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "assert (predictions == some_y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (some_tree.feature_importances_ == [0.25, 1.  ]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна проверка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=2, min_leaf_size=1)\n",
    "\n",
    "some_X = np.vstack((\n",
    "    np.random.normal(loc=(-5, -5), size=(100, 2)),\n",
    "    np.random.normal(loc=(-5, 5), size=(100, 2)),\n",
    "    np.random.normal(loc=(5, -5), size=(100, 2)),\n",
    "    np.random.normal(loc=(5, 5), size=(100, 2)),\n",
    "))\n",
    "\n",
    "some_X = np.hstack((some_X, np.random.random((400, 100))))\n",
    "\n",
    "some_y = np.array(\n",
    "    [0] * 100 + [1] * 100 + [2] * 100 + [3] * 100\n",
    ")\n",
    "\n",
    "some_tree.fit(some_X, some_y)\n",
    "predictions = some_tree.predict(some_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "assert (predictions == some_y).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Визуализация результатов\n",
    "\n",
    "Давайте проверим, что дерево работает на нескольких модельных задачах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для визуализации двумерной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(X, y, figsize=(6, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    n_classes = y.max() + 1\n",
    "    for i in range(n_classes):\n",
    "        plt.plot(X[:, 0][y == i], X[:, 1][y == i], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для визуализации работы дерева на двумерной выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_surface(clf, X, y, plot_step=0.2, cmap='Spectral', figsize=(6, 5)):\n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    n_classes = len(set(y))\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.5)    \n",
    "    y_pred = clf.predict(X)\n",
    "\n",
    "    # Plot the training points\n",
    "    plt.scatter(*X[y_pred == y].T, marker='.', s=70,\n",
    "                c=y[y_pred == y], cmap=cmap, alpha=0.9, label='correct')\n",
    "    plt.scatter(*X[y_pred != y].T, marker='x', s=50,\n",
    "                c=y[y_pred != y], cmap=cmap, label='errors')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.axis(\"tight\")\n",
    "    plt.legend(loc='best')\n",
    "    print(\"Accuracy =\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала рассмотрим простую задачу с полностью разделимыми по некоторому признаку классами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, n_features=2, cluster_std=1.5, centers=2, random_state=23)\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим к этой задаче алгоритм \"решающий пень\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=1, min_leaf_size=1)\n",
    "some_tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы всё реализовали правильно, то должно получиться что-то такое:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_surface(some_tree, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь рассмотрим более сложную задачу с плохо разделимыми классами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, n_features=2, cluster_std=2, centers=5, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результаты, которые получаются при разных значениях глубины:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=1, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=2, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=3, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=5, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=10, min_leaf_size=1)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что начиная с некоторого значения глубины, дерево начинает сильно переобучаться (новые сплиты пытаются описать максимум 1-2 объекта). Это можно поправить, изменяя параметр `min_leaf_size` или `min_improvement`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=10, min_leaf_size=10)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = ClassificationDecisionTree(max_depth=10, min_improvement=0.08)\n",
    "some_tree.fit(X, y)\n",
    "\n",
    "plot_decision_surface(some_tree, X, y, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание (10 баллов)\n",
    "\n",
    "1. (4 балла) Доделать все пункты ноутбука до конца\n",
    "\n",
    "2. (2 балла) Для регрессионного дерева необходимо использовать такой критерий:\n",
    "    $$H(R) = \\min_c \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R} (y_i - c)^2$$\n",
    "    \n",
    "    Докажите, что минимум H(R) достигается при $c$:\n",
    "\n",
    "    $$ c = \\frac{1}{|R|} \\sum_{(x_j, y_j) \\in R} y_j$$\n",
    "\n",
    "3. (2 балла) Реализуйте регрессионное дерево. В качестве критерия необходимо использовать критерий, определённый в пункте 2. В качестве функции выдачи результатов необходимо использовать среднее значение ответов по всем объектам в листе.\n",
    "\n",
    "    Сгенерируйте однопризнаковую выборку для тестирования дерева и покажите работу дерева на этой выборке. Отобразите на одном графике значения алгоритма и точки. Нарисуйте эту картинку для нескольких значений глубины. Сделайте выводы.\n",
    "    \n",
    "    \n",
    "3. (2 балла) Протестируйте ваше дерево на california_housing датасете (можно загрузить с помощью sklearn.datasets.fetch_california_housing).\n",
    "    Разбейте данные на обучение, контроль и тест. Подберите гиперпараметры по контрольной выборке, покажите качество алгоритма на тестовой. Сделайте выводы.\n",
    "\n",
    "\n",
    "Бонусных баллов в этот раз нет :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (4 балла) Доделать все пункты ноутбука до конца  \n",
    "- Done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. (2 балла) Для регрессионного дерева необходимо использовать такой критерий:\n",
    "    $$H(R) = \\min_c \\frac{1}{|R|} \\sum_{(x_i, y_i) \\in R} (y_i - c)^2$$\n",
    "    \n",
    "    Докажите, что минимум H(R) достигается при $c$:\n",
    "\n",
    "    $$ c = \\frac{1}{|R|} \\sum_{(x_j, y_j) \\in R} y_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Легко видеть, что у функции $f(c) = \\frac{1}{|R|} \\sum_{x_i, y_i} (y_i - c)^2$ есть минимум и нет максимума.  \n",
    "Найдем $ 0 = f'(c_0) = \\frac{1}{|R|} \\sum_{y_i} 2(y_i - c_0)$.  \n",
    "Следовательно $\\sum_{y_i} (y_i - c_0) = 0$  \n",
    "$c = \\frac{1}{|R|} \\sum_{y_i} y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. (2 балла) Реализуйте регрессионное дерево. В качестве критерия необходимо использовать критерий, определённый в пункте 2. В качестве функции выдачи результатов необходимо использовать среднее значение ответов по всем объектам в листе.\n",
    "\n",
    "    Сгенерируйте однопризнаковую выборку для тестирования дерева и покажите работу дерева на этой выборке. Отобразите на одном графике значения алгоритма и точки. Нарисуйте эту картинку для нескольких значений глубины. Сделайте выводы.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDecisionTree(DecisionTree):\n",
    "    def __init__(self, criterion='MSE', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if criterion not in ('MSE', ):\n",
    "            raise ValueError('Unsupported criterion', criterion)\n",
    "        self.criterion = criterion\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = np.max(y) + 1\n",
    "        self.n_features_ = X.shape[1]\n",
    "        \n",
    "        if self.criterion == 'MSE':\n",
    "            criterion = MSECriterion()\n",
    "        else:\n",
    "            raise ValueError('Unsupported criterion', criterion)\n",
    "\n",
    "        self.root_ = TreeNode.from_criterion(y, criterion)\n",
    "        self._build_nodes(X, y, criterion, np.arange(X.shape[0]), self.root_)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        self._get_nodes_predictions(X, predictions, np.arange(X.shape[0]), self.root_)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([\n",
    "    np.random.normal(loc=0, size=(100, 1)),\n",
    "    np.random.normal(loc=6, size=(100, 1)),\n",
    "    np.random.normal(loc=12, size=(100, 1)),\n",
    "])\n",
    "\n",
    "y_train = np.concatenate([\n",
    "    np.random.normal(10, size=100),\n",
    "    np.random.normal(20, size=100),\n",
    "    np.random.normal(30, size=100),\n",
    "])\n",
    "\n",
    "X_test = np.linspace(-5, 15, 100).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = RegressionDecisionTree(max_depth=1, min_leaf_size=10)\n",
    "some_tree.fit(X_train, y_train)\n",
    "y_test = some_tree.predict(X_test)\n",
    "\n",
    "plt.scatter(X_train[:, 0], y_train, label='Train')\n",
    "plt.scatter(X_test[:, 0], y_test, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('Test regression')\n",
    "plt.xlabel('X feature 0')\n",
    "plt.ylabel('target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одного вопроса очевидно недостаточно чтобы разделить три группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = RegressionDecisionTree(max_depth=2, min_leaf_size=10)\n",
    "some_tree.fit(X_train, y_train)\n",
    "y_test = some_tree.predict(X_test)\n",
    "\n",
    "plt.scatter(X_train[:, 0], y_train, label='Train')\n",
    "plt.scatter(X_test[:, 0], y_test, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('Test regression')\n",
    "plt.xlabel('X feature 0')\n",
    "plt.ylabel('target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Глубины два уже достаточно чтобы адекватно разделить группы и выделить среднее по группе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = RegressionDecisionTree(max_depth=5, min_leaf_size=10)\n",
    "some_tree.fit(X_train, y_train)\n",
    "y_test = some_tree.predict(X_test)\n",
    "\n",
    "plt.scatter(X_train[:, 0], y_train, label='Train')\n",
    "plt.scatter(X_test[:, 0], y_test, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('Test regression')\n",
    "plt.xlabel('X feature 0')\n",
    "plt.ylabel('target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель по немногу начала переобучаться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_tree = RegressionDecisionTree(max_depth=20, min_leaf_size=10)\n",
    "some_tree.fit(X_train, y_train)\n",
    "y_test = some_tree.predict(X_test)\n",
    "\n",
    "plt.scatter(X_train[:, 0], y_train, label='Train')\n",
    "plt.scatter(X_test[:, 0], y_test, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('Test regression')\n",
    "plt.xlabel('X feature 0')\n",
    "plt.ylabel('target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель сильно переобучилась, и внутри группы имеет сильный разброс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "3. (2 балла) Протестируйте ваше дерево на california_housing датасете (можно загрузить с помощью sklearn.datasets.fetch_california_housing).\n",
    "    Разбейте данные на обучение, контроль и тест. Подберите гиперпараметры по контрольной выборке, покажите качество алгоритма на тестовой. Сделайте выводы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.fetch_california_housing()\n",
    "# X = dataset['data']\n",
    "# y = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train, X_control, y_train, y_control = train_test_split(X_train, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('pca cummulative explained variance ration')\n",
    "plt.plot(np.arange(X_train.shape[1]) + 1, np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('explained variance ration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется `pca` не очень сильно помогает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем подобрать параметры и посмотреть как обучится модель  \n",
    "Запускать `optuna` мне немного лень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for _ in tqdm(range(10)):\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.arange(1, 10)\n",
    "min_leaf_sizes = np.arange(1, 100, 5)\n",
    "mse_train = np.zeros((max_depths.shape[0], min_leaf_sizes.shape[0]))\n",
    "mse_control = np.zeros((max_depths.shape[0], min_leaf_sizes.shape[0]))\n",
    "\n",
    "for i_md, md in tqdm(enumerate(max_depths)):\n",
    "    for i_mls, mls in enumerate(min_leaf_sizes):\n",
    "        model = RegressionDecisionTree(max_depth=md, min_leaf_size=mls)\n",
    "        model.fit(X_train, y_train)\n",
    "        score_train = mse(y_train, model.predict(X_train))\n",
    "        score_control = mse(y_control, model.predict(X_control))\n",
    "\n",
    "        mse_train[i_md, i_mls] = score_train\n",
    "        mse_control[i_md, i_mls] = score_control\n",
    "\n",
    "# Почему то tqdm не работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "af54d8ffe8b61cab9e1bcfcc3a5e2eeb6c71d1475cf011cf65945131006eac57"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
